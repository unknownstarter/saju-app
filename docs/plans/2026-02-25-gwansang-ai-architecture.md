# AI ê´€ìƒ (Face Reading) Feature -- Technical Architecture

> **Author**: Ari (Technical Architecture Agent)
> **Date**: 2026-02-25
> **Status**: Architecture Design (Pre-Implementation)

---

## Table of Contents

1. [Architecture Overview](#1-architecture-overview)
2. [Package Selection & Rationale](#2-package-selection--rationale)
3. [Data Flow Pipeline](#3-data-flow-pipeline)
4. [On-Device Face Analysis (MediaPipe/ML Kit)](#4-on-device-face-analysis)
5. [Face Measurement Computation for ê´€ìƒ](#5-face-measurement-computation)
6. [AI Interpretation Layer](#6-ai-interpretation-layer)
7. [Photo Quality Validation](#7-photo-quality-validation)
8. [Performance & Cost Estimates](#8-performance--cost-estimates)
9. [Flutter Implementation Architecture](#9-flutter-implementation-architecture)
10. [Privacy & Security](#10-privacy--security)
11. [Key Code Snippets](#11-key-code-snippets)
12. [Risk Analysis & Mitigations](#12-risk-analysis--mitigations)

---

## 1. Architecture Overview

### Text-Based Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FLUTTER APP (Client)                        â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Photo Layer  â”‚    â”‚  ML Kit      â”‚    â”‚  ê´€ìƒ Presentation    â”‚  â”‚
â”‚  â”‚              â”‚    â”‚  Processing   â”‚    â”‚                       â”‚  â”‚
â”‚  â”‚ image_picker â”‚â”€â”€â”€â–¶â”‚  (On-Device)  â”‚â”€â”€â”€â–¶â”‚ GwansangResultPage   â”‚  â”‚
â”‚  â”‚ image_cropperâ”‚    â”‚              â”‚    â”‚ FaceAnalysisWidget    â”‚  â”‚
â”‚  â”‚ camera       â”‚    â”‚ Face Detect  â”‚    â”‚ GwansangCard          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Contours     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â”‚ Landmarks    â”‚              â–²                â”‚
â”‚                       â”‚ Classificationâ”‚              â”‚                â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                â”‚
â”‚                              â”‚                       â”‚                â”‚
â”‚                     structured JSON            result JSON           â”‚
â”‚                     (measurements)           (interpretation)        â”‚
â”‚                              â”‚                       â”‚                â”‚
â”‚                              â–¼                       â”‚                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚                â”‚
â”‚                    â”‚  Gwansang       â”‚               â”‚                â”‚
â”‚                    â”‚  Measurement    â”‚               â”‚                â”‚
â”‚                    â”‚  Computer       â”‚               â”‚                â”‚
â”‚                    â”‚  (Pure Dart)    â”‚               â”‚                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚                â”‚
â”‚                             â”‚                        â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SUPABASE EDGE FUNCTIONS                         â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  generate-gwansang-insight                                   â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚  Input:                                                       â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ face_measurements: { ... structured JSON ... }          â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ saju_profile: { pillars, five_elements, dominant }      â”‚    â”‚
â”‚  â”‚  â””â”€â”€ user_name: string                                       â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚  Processing:                                                  â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ 1. Validate measurements                                â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ 2. Construct ê´€ìƒ + ì‚¬ì£¼ combined prompt                 â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ 3. Call Claude API (Haiku 4.5)                          â”‚    â”‚
â”‚  â”‚  â””â”€â”€ 4. Parse & return structured result                     â”‚    â”‚
â”‚  â”‚                                                               â”‚    â”‚
â”‚  â”‚  Output:                                                      â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ gwansang_reading: string (ê´€ìƒ í•´ì„ë¬¸)                   â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ face_traits: string[] (ì„±ê²© í‚¤ì›Œë“œ)                      â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ combined_insight: string (ì‚¬ì£¼+ê´€ìƒ í†µí•© ì¸ì‚¬ì´íŠ¸)        â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ love_fortune: string (ì—°ì• ìš´)                            â”‚    â”‚
â”‚  â”‚  â””â”€â”€ compatibility_hints: string[] (ê¶í•© íŒíŠ¸)               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  save-gwansang-profile                                       â”‚    â”‚
â”‚  â”‚  (measurements + interpretation â†’ gwansang_profiles table)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SUPABASE POSTGRESQL                              â”‚
â”‚                                                                     â”‚
â”‚  gwansang_profiles                                                  â”‚
â”‚  â”œâ”€â”€ id (uuid, PK)                                                  â”‚
â”‚  â”œâ”€â”€ user_id (uuid, FK â†’ profiles)                                 â”‚
â”‚  â”œâ”€â”€ face_measurements (jsonb)     -- structured measurements      â”‚
â”‚  â”œâ”€â”€ face_shape (text)             -- oval/round/square/heart/long â”‚
â”‚  â”œâ”€â”€ gwansang_reading (text)       -- AI ê´€ìƒ í•´ì„ë¬¸                â”‚
â”‚  â”œâ”€â”€ face_traits (text[])          -- ì„±ê²© í‚¤ì›Œë“œ                   â”‚
â”‚  â”œâ”€â”€ combined_insight (text)       -- ì‚¬ì£¼+ê´€ìƒ í†µí•© ì¸ì‚¬ì´íŠ¸       â”‚
â”‚  â”œâ”€â”€ love_fortune (text)           -- ì—°ì• ìš´                        â”‚
â”‚  â”œâ”€â”€ compatibility_hints (text[])  -- ê¶í•© íŒíŠ¸                    â”‚
â”‚  â”œâ”€â”€ photo_hash (text)             -- ì‚¬ì§„ ë³€ê²½ ê°ì§€ìš© í•´ì‹œ        â”‚
â”‚  â”œâ”€â”€ created_at (timestamptz)                                       â”‚
â”‚  â””â”€â”€ updated_at (timestamptz)                                       â”‚
â”‚                                                                     â”‚
â”‚  RLS: ë³¸ì¸ë§Œ ì¡°íšŒ/ìˆ˜ì • ê°€ëŠ¥                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Design Principles

1. **Privacy-First**: Raw photos NEVER leave the device. Only computed measurements (numbers/ratios) are sent to the server.
2. **On-Device ML**: Face detection and landmark extraction run entirely on-device via Google ML Kit.
3. **Coherent ì‚¬ì£¼+ê´€ìƒ**: The AI interprets face measurements in context of the user's saju profile, producing a unified narrative -- not two separate readings.
4. **Cost Efficiency**: Use Haiku 4.5 for ê´€ìƒ interpretation (sufficient quality for structured-input interpretation).

---

## 2. Package Selection & Rationale

### Primary Package: `google_mlkit_face_detection` v0.13.2

| Criterion | `google_mlkit_face_detection` | `google_mlkit_face_mesh_detection` | Direct MediaPipe |
|-----------|------|---------|--------|
| **iOS Support** | YES | NO (Android only) | Complex setup |
| **Android Support** | YES | YES | YES |
| **Landmark Count** | 10 landmarks + 15 contour types | 468 3D points | 478 points |
| **Flutter Integration** | Native plugin (pub.dev) | Native plugin (pub.dev) | Platform channels needed |
| **Maturity** | Stable, v0.13.2 | Beta, v0.4.2 | Requires custom bridges |
| **Sufficient for ê´€ìƒ?** | YES (see analysis below) | Overkill | Overkill |

**Decision: `google_mlkit_face_detection`**

Rationale:
- Cross-platform (iOS + Android) is mandatory for a dating app
- The 10 landmarks + 15 contour types provide sufficient data for all ê´€ìƒ measurements
- Face contours provide point arrays (not just single points), giving detailed shape information for eyes, eyebrows, nose, lips, jawline, and face outline
- Face Mesh (468 points) would be ideal but is Android-only -- a dealbreaker
- Direct MediaPipe requires building custom platform channels, adding maintenance burden with no significant quality gain

### Full Package List

```yaml
# pubspec.yaml additions for ê´€ìƒ feature

dependencies:
  # --- Face Analysis (On-Device ML) ---
  google_mlkit_face_detection: ^0.13.2    # Face detection + contours + landmarks
  google_mlkit_commons: ^0.8.0            # Shared types (InputImage, etc.)

  # --- Photo Capture & Processing ---
  image_picker: ^1.1.2                    # Camera/gallery photo selection
  image_cropper: ^8.0.2                   # Face-area cropping with guide overlay
  image: ^4.5.3                           # Image manipulation (resize, normalize)

  # --- Crypto (photo hashing) ---
  crypto: ^3.0.6                          # SHA-256 hash for photo change detection
```

### Contour Types Available (15 total)

These contour types each return an array of `Point<int>` values:

| Contour Type | Points | ê´€ìƒ Usage |
|---|---|---|
| `face` | ~36 points | Face shape (ì–¼êµ´í˜•), symmetry |
| `leftEye` | ~16 points | Eye shape, size |
| `rightEye` | ~16 points | Eye shape, size |
| `leftEyebrowTop` | ~5 points | Eyebrow arch, length |
| `leftEyebrowBottom` | ~5 points | Eyebrow thickness |
| `rightEyebrowTop` | ~5 points | Eyebrow arch, length |
| `rightEyebrowBottom` | ~5 points | Eyebrow thickness |
| `upperLipTop` | ~11 points | Lip shape, width |
| `upperLipBottom` | ~9 points | Lip thickness |
| `lowerLipTop` | ~9 points | Lip ratio |
| `lowerLipBottom` | ~9 points | Mouth size |
| `noseBridge` | ~2 points | Nose bridge height |
| `noseBottom` | ~3 points | Nose width, tip shape |
| `leftCheek` | ~1 point | Face width |
| `rightCheek` | ~1 point | Face width |

### Landmark Types Available (10 total)

| Landmark | ê´€ìƒ Usage |
|---|---|
| `leftEye` | Eye center position |
| `rightEye` | Eye center position, spacing |
| `leftEar` | Face width measurement |
| `rightEar` | Face width measurement |
| `leftMouth` | Mouth width |
| `rightMouth` | Mouth width |
| `bottomMouth` | Lower lip position, chin distance |
| `noseBase` | Nose position, face center |
| `leftCheek` | Cheek prominence |
| `rightCheek` | Cheek prominence |

### Classifications Available

| Classification | ê´€ìƒ Usage |
|---|---|
| `smilingProbability` | Expression baseline |
| `leftEyeOpenProbability` | Eye symmetry |
| `rightEyeOpenProbability` | Eye symmetry |
| `headEulerAngleX` | Head tilt (quality check) |
| `headEulerAngleY` | Head rotation (quality check) |
| `headEulerAngleZ` | Head tilt (quality check) |

---

## 3. Data Flow Pipeline

### End-to-End Flow

```
User taps "ê´€ìƒ ë¶„ì„í•˜ê¸°"
         â”‚
         â–¼
â”Œâ”€ STEP 1: Photo Capture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€ Camera OR Gallery â”€â”€â”    â”Œâ”€ Quality Gate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ image_picker          â”‚â”€â”€â”€â–¶â”‚ 1. Face detected?              â”‚ â”‚
â”‚  â”‚ preferredCameraDevice â”‚    â”‚ 2. Face size > 20% of image?   â”‚ â”‚
â”‚  â”‚ = CameraDevice.front  â”‚    â”‚ 3. Head angle < 15 degrees?    â”‚ â”‚
â”‚  â”‚ maxWidth: 1080        â”‚    â”‚ 4. Both eyes open?             â”‚ â”‚
â”‚  â”‚ imageQuality: 85      â”‚    â”‚ 5. Lighting adequate?          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    PASS â”‚ FAIL â†’ retry guidance  â”‚
â”‚                                          â–¼                       â”‚
â”‚                               â”Œâ”€ Face Crop â”€â”€â”€â”€â”€â”               â”‚
â”‚                               â”‚ Auto-crop to     â”‚               â”‚
â”‚                               â”‚ face bounding    â”‚               â”‚
â”‚                               â”‚ box + 30% margin â”‚               â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                        â–¼                         â”‚
â”‚                               Repeat for 3 photos               â”‚
â”‚                               (front, slight-left, slight-right) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼  (3 validated face images, on-device only)
â”Œâ”€ STEP 2: ML Kit Processing (On-Device) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For each photo:                                                  â”‚
â”‚  â”œâ”€â”€ FaceDetector.processImage(inputImage)                       â”‚
â”‚  â”œâ”€â”€ Extract: 10 landmarks, 15 contour types, classifications   â”‚
â”‚  â””â”€â”€ Store raw landmark/contour data                             â”‚
â”‚                                                                   â”‚
â”‚  Aggregate across 3 photos:                                       â”‚
â”‚  â”œâ”€â”€ Use frontal photo as PRIMARY (most reliable)                â”‚
â”‚  â”œâ”€â”€ Side photos for: nose bridge depth, jawline profile         â”‚
â”‚  â””â”€â”€ Average measurements where applicable                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼  (raw landmarks + contours for 3 photos)
â”Œâ”€ STEP 3: ê´€ìƒ Measurement Computation (On-Device, Pure Dart) â”€â”€â”€â”
â”‚  GwansangMeasurementComputer.compute(faceLandmarks) â†’            â”‚
â”‚  {                                                                â”‚
â”‚    "face_shape": "oval",                                         â”‚
â”‚    "sam_jeong": { "upper": 0.34, "middle": 0.33, "lower": 0.33 },â”‚
â”‚    "eyes": { "spacing_ratio": 0.28, "slant_angle": 3.2, ... },  â”‚
â”‚    "nose": { "bridge_ratio": 0.42, "width_ratio": 0.31, ... },  â”‚
â”‚    "mouth": { "width_ratio": 0.38, "lip_ratio": 0.55, ... },    â”‚
â”‚    "eyebrows": { "arch_height": 0.12, "thickness": 0.04, ... }, â”‚
â”‚    "forehead": { "height_ratio": 0.34, "width_ratio": 0.92 },   â”‚
â”‚    "jawline": { "angle": 125, "shape": "rounded" },             â”‚
â”‚    "symmetry": { "score": 87, "left_right_diff": 0.03 },        â”‚
â”‚    "overall_proportions": { ... }                                 â”‚
â”‚  }                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼  (structured measurements JSON -- NO photos)
â”Œâ”€ STEP 4: AI Interpretation (Server-Side) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase Edge Function: generate-gwansang-insight               â”‚
â”‚  Input:  measurements JSON + saju_profile                        â”‚
â”‚  LLM:    Claude Haiku 4.5 ($1/$5 per 1M tokens)                 â”‚
â”‚  Output: ê´€ìƒ í•´ì„, ì„±ê²© í‚¤ì›Œë“œ, ì‚¬ì£¼+ê´€ìƒ í†µí•© ì¸ì‚¬ì´íŠ¸,        â”‚
â”‚          ì—°ì• ìš´, ê¶í•© íŒíŠ¸                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€ STEP 5: Result Display & Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”œâ”€â”€ Save to gwansang_profiles table                             â”‚
â”‚  â”œâ”€â”€ Link to user profile (profiles.gwansang_profile_id)         â”‚
â”‚  â””â”€â”€ Display GwansangResultPage with animated reveal             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why 3 Photos?

| Photo | Purpose | What It Captures Best |
|---|---|---|
| **Frontal** (primary) | All measurements | Face shape, symmetry, eye/nose/mouth proportions, ì‚¼ì • |
| **Slight Left** (~15-20 deg) | Nose profile, jawline | Nose bridge height, jaw angle, ear position |
| **Slight Right** (~15-20 deg) | Cross-validation | Confirms left-side measurements, catches asymmetry |

**Aggregation Strategy**: Frontal photo is the primary data source for all 2D measurements. Side photos supplement with depth-related features (nose bridge prominence, jaw protrusion) and provide symmetry cross-validation. We do NOT average all measurements blindly -- each measurement uses the photo angle that provides the most reliable data for that specific measurement.

---

## 4. On-Device Face Analysis

### ML Kit Configuration

```dart
/// Optimal FaceDetector configuration for ê´€ìƒ analysis
final faceDetectorOptions = FaceDetectorOptions(
  enableClassification: true,    // smiling/eye-open probability
  enableLandmarks: true,         // 10 facial landmark points
  enableContours: true,          // 15 contour types (point arrays)
  enableTracking: false,         // not needed for static photos
  performanceMode: FaceDetectorMode.accurate,  // accuracy over speed
  minFaceSize: 0.25,             // face must be >= 25% of image
);
```

### Processing Pipeline

```dart
/// Process a single photo and extract face data
Future<FaceAnalysisResult?> analyzePhoto(XFile photo) async {
  // 1. Convert to InputImage
  final inputImage = InputImage.fromFilePath(photo.path);

  // 2. Detect faces
  final faces = await _faceDetector.processImage(inputImage);

  // 3. Validate: exactly 1 face
  if (faces.isEmpty) throw NoFaceDetectedException();
  if (faces.length > 1) throw MultipleFacesDetectedException();

  final face = faces.first;

  // 4. Quality checks
  _validateFaceQuality(face);

  // 5. Extract all data
  return FaceAnalysisResult(
    boundingBox: face.boundingBox,
    landmarks: _extractLandmarks(face),
    contours: _extractContours(face),
    headEulerAngleX: face.headEulerAngleX,
    headEulerAngleY: face.headEulerAngleY,
    headEulerAngleZ: face.headEulerAngleZ,
    smilingProbability: face.smilingProbability,
    leftEyeOpenProbability: face.leftEyeOpenProbability,
    rightEyeOpenProbability: face.rightEyeOpenProbability,
  );
}
```

### Expected On-Device Performance

| Metric | Estimate | Notes |
|---|---|---|
| Face detection (per photo) | 200-400ms | Accurate mode, single face |
| Landmark + contour extraction | Included above | Same pass |
| Measurement computation | <50ms | Pure Dart math |
| Total per photo | ~300-450ms | On modern devices (iPhone 13+, Pixel 6+) |
| Total for 3 photos | ~1.0-1.5s | Sequential processing |
| Memory overhead | ~30-50MB | ML model loaded in memory |

---

## 5. Face Measurement Computation for ê´€ìƒ

### ê´€ìƒí•™ Framework: ì‚¼ì •(ä¸‰åœ) + ì˜¤ê´€(äº”å®˜)

Korean physiognomy (ê´€ìƒí•™) analyzes faces through two primary frameworks:

**ì‚¼ì • (Three Zones / Three Sections)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ä¸Šåœ (ìƒì •/Upper)      â”‚  ì´ë§ˆ ìƒë‹¨ ~ ëˆˆì¹: ì´ˆë…„ìš´(~30ì„¸), ë¶€ëª¨ê¶
â”‚     Forehead to Eyebrows   â”‚  ì§€í˜œ, í•™ì—…, ê°€ë¬¸
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ä¸­åœ (ì¤‘ì •/Middle)      â”‚  ëˆˆì¹ ~ ì½” ë: ì¤‘ë…„ìš´(30~50ì„¸)
â”‚     Eyebrows to Nose Tip   â”‚  ì‚¬ì—…, ì˜ì§€, ê²°ë‹¨ë ¥
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ä¸‹åœ (í•˜ì •/Lower)       â”‚  ì½” ë ~ í„±: ë§Œë…„ìš´(50ì„¸~), ìë…€ê¶
â”‚     Nose Tip to Chin        â”‚  ì¬ë¬¼, ê±´ê°•, ë³µë•
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ideal: 1:1:1 ratio (ê· í˜•ì¡íŒ ì‚¼ì • = ê³ ë¥¸ ì¸ìƒìš´)
```

**ì˜¤ê´€ (Five Features / Five Organs)**
1. **ê·€ (Ears)** -- ì±„ì²­ê´€(é‡‡è½å®˜): ì§€í˜œ, ì¥ìˆ˜
2. **ëˆˆì¹ (Eyebrows)** -- ë³´ìˆ˜ê´€(ä¿å£½å®˜): ê±´ê°•, ìˆ˜ëª…
3. **ëˆˆ (Eyes)** -- ê°ì°°ê´€(ç›£å¯Ÿå®˜): ë¶€ê·€ë¹ˆì²œ (ì–¼êµ´ì´ ì²œ ëƒ¥ì´ë©´ ëˆˆì´ êµ¬ë°± ëƒ¥)
4. **ì½” (Nose)** -- ì‹¬íŒê´€(å¯©åˆ¤å®˜): ì¬ë¬¼, ìì¡´ì‹¬
5. **ì… (Mouth)** -- ì¶œë‚©ê´€(å‡ºç´å®˜): ì‹ë¡, í‘œí˜„ë ¥

### Measurement Computation: Contour Points to ê´€ìƒ Metrics

#### Output JSON Schema: `GwansangMeasurements`

```json
{
  "version": "1.0",
  "source_photos": 3,
  "primary_photo_index": 0,

  "face_shape": {
    "type": "oval",
    "confidence": 0.82,
    "width_height_ratio": 0.74,
    "jaw_forehead_ratio": 0.88,
    "description": "ë‹¬ê±€í˜•"
  },

  "sam_jeong": {
    "upper_ratio": 0.34,
    "middle_ratio": 0.33,
    "lower_ratio": 0.33,
    "balance_score": 95,
    "dominant_zone": "upper"
  },

  "eyes": {
    "left_width": 42,
    "right_width": 41,
    "spacing_ratio": 0.28,
    "slant_angle_left": 3.2,
    "slant_angle_right": 2.8,
    "size_category": "medium",
    "shape": "almond",
    "open_ratio_left": 0.35,
    "open_ratio_right": 0.36,
    "symmetry_score": 92
  },

  "eyebrows": {
    "left_length": 58,
    "right_length": 57,
    "arch_height_left": 8,
    "arch_height_right": 7,
    "thickness_left": 6,
    "thickness_right": 6,
    "shape": "natural_arch",
    "spacing_from_eye": 12,
    "symmetry_score": 90
  },

  "nose": {
    "bridge_length": 48,
    "bridge_width": 14,
    "tip_width": 32,
    "bridge_height_ratio": 0.42,
    "width_ratio": 0.31,
    "tip_shape": "rounded",
    "profile_angle": null
  },

  "mouth": {
    "width": 52,
    "upper_lip_height": 8,
    "lower_lip_height": 10,
    "lip_ratio": 0.44,
    "width_ratio": 0.38,
    "shape": "balanced",
    "corner_angle": 2.1
  },

  "forehead": {
    "height": 68,
    "width": 128,
    "height_ratio": 0.34,
    "width_to_face_ratio": 0.92,
    "shape": "rounded"
  },

  "jawline": {
    "width": 112,
    "angle_left": 125,
    "angle_right": 127,
    "shape": "rounded",
    "chin_prominence": "moderate",
    "jaw_to_forehead_ratio": 0.88
  },

  "symmetry": {
    "overall_score": 87,
    "eye_symmetry": 92,
    "eyebrow_symmetry": 90,
    "mouth_symmetry": 95,
    "face_centerline_deviation": 0.02
  },

  "proportions": {
    "golden_ratio_score": 78,
    "eye_spacing_to_face_width": 0.28,
    "nose_width_to_face_width": 0.24,
    "mouth_width_to_face_width": 0.38,
    "face_height_to_width": 1.35
  }
}
```

### Face Shape Classification Algorithm

```dart
/// Classify face shape from face contour points
FaceShapeType classifyFaceShape({
  required List<Point<int>> faceContour,  // ~36 points outlining face
  required Rect boundingBox,
}) {
  final faceWidth = boundingBox.width;
  final faceHeight = boundingBox.height;
  final ratio = faceWidth / faceHeight;

  // Measure jaw width vs forehead width
  // Face contour points go clockwise from chin
  final jawWidth = _measureWidthAtPercentage(faceContour, 0.75);  // 75% down
  final foreheadWidth = _measureWidthAtPercentage(faceContour, 0.15);  // 15% down
  final cheekWidth = _measureWidthAtPercentage(faceContour, 0.50);  // 50% down

  final jawToForeheadRatio = jawWidth / foreheadWidth;
  final cheekToJawRatio = cheekWidth / jawWidth;

  // Classification logic
  if (ratio > 0.85) {
    // Wide face
    if (jawToForeheadRatio > 0.95) return FaceShapeType.square;    // ì‚¬ê°í˜•
    if (jawToForeheadRatio > 0.85) return FaceShapeType.round;     // ë‘¥ê·¼í˜•
  }

  if (ratio < 0.70) return FaceShapeType.long;                      // ê¸´ ì–¼êµ´í˜•

  if (jawToForeheadRatio < 0.75) return FaceShapeType.heart;       // í•˜íŠ¸í˜• (Vë¼ì¸)

  if (cheekToJawRatio > 1.15 && jawToForeheadRatio < 0.85) {
    return FaceShapeType.diamond;                                    // ë‹¤ì´ì•„ëª¬ë“œí˜•
  }

  return FaceShapeType.oval;                                        // ë‹¬ê±€í˜• (ê¸°ë³¸)
}
```

### ì‚¼ì • (Three Zones) Computation

```dart
/// Compute ì‚¼ì • (Three Zone) proportions
SamJeongMeasurement computeSamJeong({
  required List<Point<int>> faceContour,
  required List<Point<int>> leftEyebrowTop,
  required List<Point<int>> rightEyebrowTop,
  required List<Point<int>> noseBottom,
  required Rect boundingBox,
}) {
  // Top of face (from face contour)
  final faceTop = faceContour
      .map((p) => p.y)
      .reduce((a, b) => a < b ? a : b);

  // Eyebrow line (average of left and right eyebrow tops)
  final eyebrowY = [
    ...leftEyebrowTop.map((p) => p.y),
    ...rightEyebrowTop.map((p) => p.y),
  ].reduce((a, b) => a + b) ~/ (leftEyebrowTop.length + rightEyebrowTop.length);

  // Nose bottom (average y of nose bottom contour)
  final noseBottomY = noseBottom
      .map((p) => p.y)
      .reduce((a, b) => a + b) ~/ noseBottom.length;

  // Chin (bottom of face contour)
  final chinY = faceContour
      .map((p) => p.y)
      .reduce((a, b) => a > b ? a : b);

  final totalHeight = (chinY - faceTop).toDouble();
  final upperHeight = (eyebrowY - faceTop).toDouble();
  final middleHeight = (noseBottomY - eyebrowY).toDouble();
  final lowerHeight = (chinY - noseBottomY).toDouble();

  return SamJeongMeasurement(
    upperRatio: upperHeight / totalHeight,
    middleRatio: middleHeight / totalHeight,
    lowerRatio: lowerHeight / totalHeight,
    balanceScore: _computeBalanceScore(upperHeight, middleHeight, lowerHeight),
  );
}

/// Balance score: 100 = perfect 1:1:1, lower = more imbalanced
int _computeBalanceScore(double upper, double middle, double lower) {
  final total = upper + middle + lower;
  final ideal = total / 3;
  final deviation = (upper - ideal).abs() + (middle - ideal).abs() + (lower - ideal).abs();
  final maxDeviation = total * 2 / 3;  // theoretical max
  return ((1 - deviation / maxDeviation) * 100).round().clamp(0, 100);
}
```

### Mapping Contour Points to ê´€ìƒ Measurements

| ê´€ìƒ Measurement | Contour Types Used | Computation |
|---|---|---|
| Face shape (ì–¼êµ´í˜•) | `face` | Width-height ratio + jaw/forehead/cheek width ratios |
| ì‚¼ì • balance | `face`, `leftEyebrowTop`, `rightEyebrowTop`, `noseBottom` | Y-coordinate ratios of upper/middle/lower zones |
| Eye spacing | `leftEye`, `rightEye` | Distance between inner corners / face width |
| Eye shape | `leftEye`, `rightEye` | Width/height ratio, slant angle from inner to outer corner |
| Eye size | `leftEye`, `rightEye` | Eye width / face width |
| Eyebrow arch | `leftEyebrowTop`, `leftEyebrowBottom` | Max height above eye / eyebrow length |
| Eyebrow thickness | `leftEyebrowTop`, `leftEyebrowBottom` | Average vertical distance between top and bottom contours |
| Nose bridge | `noseBridge` | Length from forehead to nose tip |
| Nose width | `noseBottom` | Width of nose bottom contour |
| Mouth width | `upperLipTop` or landmarks: `leftMouth`, `rightMouth` | Distance between mouth corners / face width |
| Lip ratio | `upperLipBottom`, `lowerLipTop` | Upper lip height / total lip height |
| Forehead height | `face`, `leftEyebrowTop` | Distance from face top to eyebrow line |
| Jawline shape | `face` (lower portion) | Angle and curvature of lower face contour |
| Symmetry | All bilateral contours | Left-right mirroring deviation |

---

## 6. AI Interpretation Layer

### Model Selection: Claude Haiku 4.5

**Rationale**:

| Factor | Haiku 4.5 | Sonnet 4.5 |
|---|---|---|
| Cost (input) | $1/M tokens | $3/M tokens |
| Cost (output) | $5/M tokens | $15/M tokens |
| Quality for structured input | Excellent | Better but unnecessary |
| Latency | ~1-2s | ~2-4s |
| **Cost per ê´€ìƒ reading** | **~$0.003-0.005** | **~$0.009-0.015** |

Haiku 4.5 is the clear winner because:
1. Input is highly structured JSON (not ambiguous natural language)
2. The model follows a detailed prompt template -- creativity is guided
3. Quality difference between Haiku and Sonnet on structured interpretation tasks is minimal (<5%)
4. At scale (100K users), cost difference: Haiku ~$400 vs Sonnet ~$1,200

**Fallback**: If Haiku quality proves insufficient during testing, upgrade to Sonnet 4.5. The Edge Function can be switched without client changes.

### Token Estimates Per Request

```
Input:
  System prompt (ê´€ìƒ interpretation instructions):   ~800 tokens
  Face measurements JSON:                             ~400 tokens
  Saju profile data:                                  ~200 tokens
  Combined prompt:                                    ~300 tokens
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total input:                                       ~1,700 tokens

Output:
  ê´€ìƒ í•´ì„ë¬¸ (gwansang_reading):                     ~400 tokens
  ì„±ê²© í‚¤ì›Œë“œ (face_traits, 5-8 items):               ~50 tokens
  ì‚¬ì£¼+ê´€ìƒ í†µí•© ì¸ì‚¬ì´íŠ¸ (combined_insight):          ~300 tokens
  ì—°ì• ìš´ (love_fortune):                              ~200 tokens
  ê¶í•© íŒíŠ¸ (compatibility_hints, 3-5 items):         ~80 tokens
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total output:                                      ~1,030 tokens
```

### Cost Per Analysis

```
Haiku 4.5:
  Input:  1,700 tokens * $1.00/1M  = $0.0017
  Output: 1,030 tokens * $5.00/1M  = $0.0052
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total per analysis:               $0.0069 (~0.7 cents)

With prompt caching (system prompt cached):
  Cached input:  800 tokens * $0.10/1M = $0.00008 (90% off)
  Fresh input:   900 tokens * $1.00/1M = $0.0009
  Output:       1,030 tokens * $5.00/1M = $0.0052
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total with caching:                $0.0062 (~0.6 cents)

Batch processing (if applicable, 50% off):
  Total with batch:                  $0.0035 (~0.35 cents)
```

### Prompt Engineering

The prompt must produce a ê´€ìƒ reading that:
1. Feels authentic and rooted in traditional Korean physiognomy
2. Coherently integrates with the user's ì‚¬ì£¼ profile
3. Is positive and empowering (dating app context -- no doom-and-gloom)
4. Highlights compatibility-relevant traits

```
Edge Function: generate-gwansang-insight

System Prompt (cached):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë‹¹ì‹ ì€ í•œêµ­ ì „í†µ ê´€ìƒí•™(è§€ç›¸å­¸)ì— ì •í†µí•œ AI ê´€ìƒê°€ì…ë‹ˆë‹¤.

## ì—­í• 
ì‚¬ìš©ìì˜ ì–¼êµ´ ì¸¡ì • ë°ì´í„°ì™€ ì‚¬ì£¼(å››æŸ±) ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬
ë”°ëœ»í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ê´€ìƒ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

## ê´€ìƒí•™ í•µì‹¬ ì›ì¹™
1. **ì‚¼ì •(ä¸‰åœ)**: ìƒì •(ì´ë§ˆ~ëˆˆì¹)ì€ ì´ˆë…„ìš´, ì¤‘ì •(ëˆˆì¹~ì½”)ì€ ì¤‘ë…„ìš´, í•˜ì •(ì½”~í„±)ì€ ë§Œë…„ìš´
2. **ì˜¤ê´€(äº”å®˜)**: ê·€(ì±„ì²­ê´€), ëˆˆì¹(ë³´ìˆ˜ê´€), ëˆˆ(ê°ì°°ê´€), ì½”(ì‹¬íŒê´€), ì…(ì¶œë‚©ê´€)
3. **ì˜¤í–‰ ì—°ê²°**: ê´€ìƒì˜ íŠ¹ì§•ì„ ì‚¬ì£¼ ì˜¤í–‰ê³¼ ì—°ê²°í•˜ì—¬ ì¼ê´€ëœ í•´ì„ì„ ì œê³µ
4. **ê· í˜•ì˜ ì›ë¦¬**: í¸í–¥ë³´ë‹¤ ê· í˜•ì´ ì¢‹ê³ , ì¡°í™”ë¡œìš´ ì´ëª©êµ¬ë¹„ê°€ ê¸¸ìƒ

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "gwansang_reading": "ê´€ìƒ í•´ì„ë¬¸ (3-4 ë¬¸ë‹¨, ì‚¼ì •ê³¼ ì˜¤ê´€ì„ ì•„ìš°ë¥´ëŠ” ì¢…í•© í•´ì„)",
  "face_traits": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],  // 5-8ê°œ
  "combined_insight": "ì‚¬ì£¼+ê´€ìƒ í†µí•© ì¸ì‚¬ì´íŠ¸ (ì‚¬ì£¼ì˜ ì˜¤í–‰ê³¼ ê´€ìƒì´ ì–´ë–»ê²Œ ì¡°í™”/ë³´ì™„ë˜ëŠ”ì§€)",
  "love_fortune": "ì—°ì• ìš´ (ê´€ìƒì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ì—°ì•  ì„±í–¥ê³¼ ì´ìƒí˜•)",
  "compatibility_hints": ["íŒíŠ¸1", "íŒíŠ¸2", ...]  // 3-5ê°œ, ë§¤ì¹­ì— í™œìš©ë  íŒíŠ¸
}

## í†¤ & ìŠ¤íƒ€ì¼
- ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ (ì†Œê°œíŒ… ì•± ë§¥ë½)
- ì „í†µ ê´€ìƒí•™ ìš©ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ ì‹ ë¢°ê° ë¶€ì—¬
- ë¶€ì •ì  íŠ¹ì§•ë„ ê¸ì •ì  ê´€ì ìœ¼ë¡œ ì¬í•´ì„ (ì˜ˆ: "í„±ì´ ê°ì§„ ê²ƒì€ ì˜ì§€ì™€ ê²°ë‹¨ë ¥ì˜ ìƒ")
- êµ¬ì²´ì ì´ê³  ê°œì¸í™”ëœ í‘œí˜„ (ì¼ë°˜ë¡  ê¸ˆì§€)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Prompt:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
## ê´€ìƒ ë¶„ì„ ìš”ì²­

### ì–¼êµ´ ì¸¡ì • ë°ì´í„°
{face_measurements JSON}

### ì‚¬ì£¼ í”„ë¡œí•„
- ì—°ì£¼: {yearPillar}
- ì›”ì£¼: {monthPillar}
- ì¼ì£¼: {dayPillar} (ì¼ê°„: {dayStem})
- ì‹œì£¼: {hourPillar}
- ì˜¤í–‰ ë¶„í¬: ëª©({wood}) í™”({fire}) í† ({earth}) ê¸ˆ({metal}) ìˆ˜({water})
- ì£¼ë„ ì˜¤í–‰: {dominantElement}
- ì‚¬ì£¼ ì„±ê²© í‚¤ì›Œë“œ: {personalityTraits}

### ë¶„ì„ ìš”ì²­
ìœ„ ì–¼êµ´ ì¸¡ì • ë°ì´í„°ì™€ ì‚¬ì£¼ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ê´€ìƒ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì‚¼ì • ë¹„ìœ¨ì— ê¸°ë°˜í•œ ì¸ìƒ ìš´ì„¸ íë¦„
2. ì˜¤ê´€ ê°ê°ì˜ íŠ¹ì§•ê³¼ ì˜ë¯¸
3. ì‚¬ì£¼ ì˜¤í–‰ê³¼ ê´€ìƒ íŠ¹ì§•ì˜ ì¡°í™”/ë³´ì™„ ê´€ê³„
4. ì—°ì• /ê¶í•© ê´€ì ì˜ ì¸ì‚¬ì´íŠ¸
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### ì‚¬ì£¼+ê´€ìƒ Coherent Combination Strategy

The key to making the combined reading feel authentic (not two separate readings stitched together):

1. **ì˜¤í–‰ Bridge**: The user's dominant ì˜¤í–‰ from ì‚¬ì£¼ becomes the interpretive lens for facial features.
   - Example: ëª©(æœ¨) dominant in saju + long face â†’ "ë‚˜ë¬´ì²˜ëŸ¼ ê³§ê³  ì­‰ ë»—ì€ ì´ëª©êµ¬ë¹„ëŠ” ëª©(æœ¨)ì´ ê°•í•œ ì‚¬ì£¼ì™€ ì™„ë²½í•œ ì¡°í™”. ì„±ì¥ì§€í–¥ì  ì„±ê²©ì´ ì–¼êµ´ì—ë„ ë“œëŸ¬ë‚©ë‹ˆë‹¤."

2. **Complementary Narrative**: Where saju and gwansang agree, reinforce. Where they differ, frame as "balance."
   - Example: Saju says í™”(ç«) = passionate, but face shows calm eyes â†’ "ëœ¨ê±°ìš´ ì—´ì •(í™”ê¸°)ì„ ì°¨ë¶„í•œ ëˆˆë§¤ê°€ ë‹¤ìŠ¤ë ¤, ê°ì •ì„ ì˜ ì¡°ì ˆí•˜ëŠ” ì„±ìˆ™í•œ ì‚¬ëŒ"

3. **ì—°ì• ìš´ Integration**: Combine saju's ì¼ì£¼ (spouse palace) interpretation with gwansang's mouth/eye features for love fortune.

---

## 7. Photo Quality Validation

### Quality Gate Checks (Before Processing)

```dart
class PhotoQualityValidator {
  /// Validates a photo for ê´€ìƒ analysis suitability
  PhotoQualityResult validate(Face face, Size imageSize) {
    final issues = <PhotoQualityIssue>[];

    // 1. Face size check: must be >= 20% of image area
    final faceArea = face.boundingBox.width * face.boundingBox.height;
    final imageArea = imageSize.width * imageSize.height;
    if (faceArea / imageArea < 0.20) {
      issues.add(PhotoQualityIssue.faceTooSmall);
    }

    // 2. Head angle check: must be within +-15 degrees
    if ((face.headEulerAngleY ?? 0).abs() > 15) {
      issues.add(PhotoQualityIssue.headTurnedTooMuch);
    }
    if ((face.headEulerAngleZ ?? 0).abs() > 10) {
      issues.add(PhotoQualityIssue.headTiltedTooMuch);
    }
    // For frontal photo, stricter: Y < 8 degrees
    // For side photos, Y should be 10-25 degrees

    // 3. Eyes open check
    final leftEyeOpen = face.leftEyeOpenProbability ?? 0;
    final rightEyeOpen = face.rightEyeOpenProbability ?? 0;
    if (leftEyeOpen < 0.5 || rightEyeOpen < 0.5) {
      issues.add(PhotoQualityIssue.eyesClosed);
    }

    // 4. Face completeness: bounding box should be fully within image
    final bbox = face.boundingBox;
    if (bbox.left < 0 || bbox.top < 0 ||
        bbox.right > imageSize.width || bbox.bottom > imageSize.height) {
      issues.add(PhotoQualityIssue.facePartiallyOutOfFrame);
    }

    // 5. Contour completeness: critical contours must be present
    if (face.contours[FaceContourType.face] == null ||
        face.contours[FaceContourType.leftEye] == null ||
        face.contours[FaceContourType.rightEye] == null ||
        face.contours[FaceContourType.noseBridge] == null) {
      issues.add(PhotoQualityIssue.insufficientContourData);
    }

    return PhotoQualityResult(
      isAcceptable: issues.isEmpty,
      issues: issues,
    );
  }
}
```

### User Guidance Messages

```dart
const qualityGuidanceMessages = {
  PhotoQualityIssue.faceTooSmall: 'ì–¼êµ´ì´ ë„ˆë¬´ ì‘ì•„ìš”. ì¡°ê¸ˆ ë” ê°€ê¹Œì´ ì™€ì£¼ì„¸ìš”!',
  PhotoQualityIssue.headTurnedTooMuch: 'ì–¼êµ´ì„ ì •ë©´ìœ¼ë¡œ í–¥í•´ì£¼ì„¸ìš”.',
  PhotoQualityIssue.headTiltedTooMuch: 'ê³ ê°œë¥¼ ë˜‘ë°”ë¡œ ì„¸ì›Œì£¼ì„¸ìš”.',
  PhotoQualityIssue.eyesClosed: 'ëˆˆì„ ëœ¨ê³  ì°ì–´ì£¼ì„¸ìš”!',
  PhotoQualityIssue.facePartiallyOutOfFrame: 'ì–¼êµ´ ì „ì²´ê°€ í™”ë©´ ì•ˆì— ë“¤ì–´ì˜¤ê²Œ í•´ì£¼ì„¸ìš”.',
  PhotoQualityIssue.insufficientContourData: 'ì¡°ëª…ì´ ë¶€ì¡±í•´ìš”. ë°ì€ ê³³ì—ì„œ ë‹¤ì‹œ ì´¬ì˜í•´ì£¼ì„¸ìš”.',
  PhotoQualityIssue.noFaceDetected: 'ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ë‹¤ì‹œ ì´¬ì˜í•´ì£¼ì„¸ìš”.',
  PhotoQualityIssue.multipleFaces: 'í•œ ëª…ë§Œ ì´¬ì˜í•´ì£¼ì„¸ìš”!',
};
```

### Photo Guidance UI Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ê´€ìƒ ë¶„ì„í•˜ê¸°                 â”‚
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                    â”‚   â”‚
â”‚  â”‚     [Face Outline Guide]           â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚     â”‚                  â”‚           â”‚   â”‚
â”‚  â”‚     â”‚   (oval guide    â”‚           â”‚   â”‚
â”‚  â”‚     â”‚    overlay)      â”‚           â”‚   â”‚
â”‚  â”‚     â”‚                  â”‚           â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                           â”‚
â”‚  ğŸ“¸ ì •ë©´ ì‚¬ì§„ (1/3)                       â”‚
â”‚  "ì–¼êµ´ì„ ê°€ì´ë“œë¼ì¸ ì•ˆì— ë§ì¶°ì£¼ì„¸ìš”"       â”‚
â”‚                                           â”‚
â”‚  [ ì¹´ë©”ë¼ë¡œ ì´¬ì˜ ]  [ ì•¨ë²”ì—ì„œ ì„ íƒ ]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Performance & Cost Estimates

### Time Breakdown (Per User Onboarding)

| Step | Duration | Where |
|---|---|---|
| Photo capture (3 photos) | ~30-60s | User action |
| Quality validation (3 photos) | ~1.5s | On-device |
| Face measurement computation | ~0.2s | On-device |
| Network roundtrip to Edge Function | ~0.5s | Network |
| Claude API call (Haiku 4.5) | ~1.5-3s | Server |
| DB save | ~0.2s | Server |
| **Total processing time** | **~4-5.5s** | |
| **Total including user action** | **~35-65s** | |

### Cost Breakdown Per User Onboarding

| Component | Cost | Notes |
|---|---|---|
| ML Kit (on-device) | $0.00 | Free, runs on device |
| Supabase Edge Function | ~$0.0001 | Pay-per-invocation |
| Claude Haiku 4.5 API | ~$0.007 | 1,700 input + 1,030 output tokens |
| Supabase DB storage | ~$0.00001 | ~2KB per row |
| **Total per user** | **~$0.007** | **Less than 1 cent** |

### Cost at Scale

| Users | Claude API Cost | Monthly (if all new) |
|---|---|---|
| 1,000 | $7 | $7 |
| 10,000 | $70 | $70 |
| 100,000 | $700 | $700 |
| 1,000,000 | $7,000 | $7,000 |

These costs are extremely manageable. Even at 1M users, the Claude API cost for ê´€ìƒ analysis is only $7,000 -- and this is a one-time cost per user (not recurring), since results are cached.

### Caching Strategy

ê´€ìƒ results are deterministic for the same photos + birth info:

```
Cache Key = SHA-256(photo1_hash + photo2_hash + photo3_hash + birth_datetime)
```

- **When to re-analyze**: Only if user uploads new photos
- **Photo change detection**: Store SHA-256 hash of photo bytes in `gwansang_profiles.photo_hash`
- **Result persistence**: Stored permanently in PostgreSQL (no TTL needed)
- **Saju change**: If user corrects birth time, re-run ê´€ìƒ analysis with same face measurements but updated saju profile (only the AI interpretation step is re-run, not face analysis)

---

## 9. Flutter Implementation Architecture

### Feature Directory Structure

```
lib/features/gwansang/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â””â”€â”€ gwansang_remote_datasource.dart     # Edge Function calls
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ face_analysis_result_model.dart     # ML Kit raw results
â”‚   â”‚   â”œâ”€â”€ gwansang_measurements_model.dart    # Computed measurements
â”‚   â”‚   â””â”€â”€ gwansang_profile_model.dart         # Full profile (measurements + AI)
â”‚   â””â”€â”€ repositories/
â”‚       â””â”€â”€ gwansang_repository_impl.dart       # Repository implementation
â”‚
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”œâ”€â”€ face_analysis_result.dart           # Raw face data entity
â”‚   â”‚   â”œâ”€â”€ gwansang_measurements.dart          # Computed measurements entity
â”‚   â”‚   â””â”€â”€ gwansang_profile.dart               # Full gwansang profile entity
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â””â”€â”€ gwansang_repository.dart            # Repository interface (abstract)
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ face_analyzer_service.dart          # ML Kit face detection wrapper
â”‚       â”œâ”€â”€ gwansang_computer.dart              # Pure Dart measurement computation
â”‚       â””â”€â”€ photo_quality_validator.dart        # Photo quality checks
â”‚
â””â”€â”€ presentation/
    â”œâ”€â”€ pages/
    â”‚   â”œâ”€â”€ gwansang_capture_page.dart          # Photo capture flow (3 photos)
    â”‚   â”œâ”€â”€ gwansang_analysis_page.dart         # Processing animation
    â”‚   â””â”€â”€ gwansang_result_page.dart           # Result display
    â”œâ”€â”€ providers/
    â”‚   â”œâ”€â”€ gwansang_provider.dart              # Main state management
    â”‚   â””â”€â”€ gwansang_provider.g.dart
    â””â”€â”€ widgets/
        â”œâ”€â”€ face_guide_overlay.dart             # Camera face guide
        â”œâ”€â”€ photo_quality_feedback.dart         # Quality issue messages
        â”œâ”€â”€ gwansang_card.dart                  # Result card (shareable)
        â””â”€â”€ sam_jeong_chart.dart                # ì‚¼ì • visualization
```

### State Management (Riverpod)

```dart
// lib/features/gwansang/presentation/providers/gwansang_provider.dart

@riverpod
class GwansangAnalysis extends _$GwansangAnalysis {
  @override
  FutureOr<GwansangProfile?> build() => null;

  /// Run the full ê´€ìƒ analysis pipeline
  Future<void> analyze({
    required List<XFile> photos,  // 3 photos
    required SajuProfile sajuProfile,
    String? userName,
  }) async {
    state = const AsyncLoading();

    try {
      final repository = ref.read(gwansangRepositoryProvider);

      // Step 1: Analyze photos on-device
      final faceResults = <FaceAnalysisResult>[];
      for (final photo in photos) {
        final result = await repository.analyzePhoto(photo);
        if (result == null) throw GwansangException('ì–¼êµ´ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
        faceResults.add(result);
      }

      // Step 2: Compute measurements (on-device, pure Dart)
      final measurements = repository.computeMeasurements(
        faceResults: faceResults,
        primaryPhotoIndex: 0,  // frontal
      );

      // Step 3: Get AI interpretation (server-side)
      final profile = await repository.generateInterpretation(
        measurements: measurements,
        sajuProfile: sajuProfile,
        userName: userName,
      );

      // Step 4: Save to DB
      await repository.saveProfile(
        userId: sajuProfile.userId,
        profile: profile,
        photoHash: _computePhotoHash(photos),
      );

      state = AsyncData(profile);
    } catch (e, st) {
      state = AsyncError(e, st);
    }
  }
}
```

### DI Registration (core/di/providers.dart addition)

```dart
// =============================================================================
// Gwansang (ê´€ìƒ)
// =============================================================================

/// ê´€ìƒ Face Analyzer Service Provider
@riverpod
FaceAnalyzerService faceAnalyzerService(Ref ref) {
  return FaceAnalyzerService();
}

/// ê´€ìƒ Measurement Computer Provider
@riverpod
GwansangComputer gwansangComputer(Ref ref) {
  return GwansangComputer();
}

/// ê´€ìƒ Remote Datasource Provider
@riverpod
GwansangRemoteDatasource gwansangRemoteDatasource(Ref ref) {
  return GwansangRemoteDatasource(ref.watch(supabaseHelperProvider));
}

/// ê´€ìƒ Repository Provider
@riverpod
GwansangRepository gwansangRepository(Ref ref) {
  return GwansangRepositoryImpl(
    faceAnalyzer: ref.watch(faceAnalyzerServiceProvider),
    computer: ref.watch(gwansangComputerProvider),
    remoteDatasource: ref.watch(gwansangRemoteDatasourceProvider),
  );
}
```

### Routing Integration

```dart
// Addition to app_router.dart
// Place after saju result route, before matching profile route

// ê´€ìƒ ë¶„ì„ (ì‚¬ì§„ ì´¬ì˜)
GoRoute(
  path: RoutePaths.gwansangCapture,
  name: RouteNames.gwansangCapture,
  builder: (context, state) {
    final sajuProfile = state.extra as SajuProfile?;
    return GwansangCapturePage(sajuProfile: sajuProfile);
  },
),

// ê´€ìƒ ë¶„ì„ ì¤‘ (ë¡œë”© ì• ë‹ˆë©”ì´ì…˜)
GoRoute(
  path: RoutePaths.gwansangAnalysis,
  name: RouteNames.gwansangAnalysis,
  builder: (context, state) {
    final data = state.extra as Map<String, dynamic>;
    return GwansangAnalysisPage(analysisData: data);
  },
),

// ê´€ìƒ ê²°ê³¼
GoRoute(
  path: RoutePaths.gwansangResult,
  name: RouteNames.gwansangResult,
  builder: (context, state) {
    final result = state.extra as GwansangProfile?;
    return GwansangResultPage(result: result);
  },
),
```

### Onboarding Flow Integration

The ê´€ìƒ feature slots into the existing onboarding flow after saju analysis:

```
Current Flow:
  Login â†’ Onboarding Form â†’ Saju Analysis â†’ Saju Result â†’ Matching Profile â†’ Home

New Flow (with ê´€ìƒ):
  Login â†’ Onboarding Form â†’ Saju Analysis â†’ Saju Result
                                                  â”‚
                                                  â–¼
                                          ê´€ìƒ Capture (3 photos)
                                                  â”‚
                                                  â–¼
                                          ê´€ìƒ Analysis (loading)
                                                  â”‚
                                                  â–¼
                                          ê´€ìƒ Result (ì‚¬ì£¼+ê´€ìƒ í†µí•©)
                                                  â”‚
                                                  â–¼
                                          Matching Profile â†’ Home
```

The ê´€ìƒ step is positioned AFTER saju result because:
1. User has already invested time and seen a "wow moment" (ì‚¬ì£¼ ê²°ê³¼)
2. ê´€ìƒ analysis requires the saju profile as input for coherent interpretation
3. Natural progression: "Now that we know your inner destiny (ì‚¬ì£¼), let's read your outer destiny (ê´€ìƒ)"

---

## 10. Privacy & Security

### Privacy-First Architecture

```
                    DEVICE BOUNDARY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  Photos â”€â”€â–º ML Kit â”€â”€â–º Measurements (JSON)  â”‚
â”‚    â–²                         â”‚               â”‚
â”‚    â”‚                         â”‚               â”‚
â”‚  NEVER leaves device         â–¼               â”‚
â”‚                     Only structured numbers  â”‚
â”‚                     leave the device â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Server
â”‚                                              â”‚
â”‚  Photos are NOT:                             â”‚
â”‚  - Uploaded to any server                    â”‚
â”‚  - Stored persistently (temp cache only)     â”‚
â”‚  - Sent to Claude API                        â”‚
â”‚  - Accessible to other users                 â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Data Crosses the Network

| Data | Sent to Server? | Content |
|---|---|---|
| Raw photos | NEVER | Stay on device only |
| Face measurements | YES | Numeric ratios, angles, categories (e.g., "oval", 0.34) |
| Saju profile | YES (already on server) | Birth date, pillars, five elements |
| AI interpretation | YES (response) | Text reading, keywords |

### Key Privacy Measures

1. **No photo upload**: Raw photos stay on-device. Only computed measurements (numbers) cross the network.
2. **Photo hash only**: We store a SHA-256 hash of photos (for change detection), not the photos themselves.
3. **Temp file cleanup**: After ML Kit processing, temporary photo files are deleted.
4. **Measurement anonymity**: The measurements JSON alone cannot reconstruct a face image.
5. **RLS protection**: PostgreSQL Row-Level Security ensures only the user can access their gwansang profile.
6. **Edge Function isolation**: The Claude API call happens in a Supabase Edge Function (server-side), so the user's API key is never exposed.
7. **No biometric storage**: Face measurements are ê´€ìƒ analysis data, not biometric identifiers. They cannot be used for facial recognition.

### Consent & Transparency

```dart
/// Show privacy consent before starting ê´€ìƒ analysis
Widget _buildPrivacyConsent() {
  return AlertDialog(
    title: const Text('ê´€ìƒ ë¶„ì„ ì•ˆë‚´'),
    content: const Column(
      mainAxisSize: MainAxisSize.min,
      crossAxisAlignment: CrossAxisAlignment.start,
      children: [
        Text('ê´€ìƒ ë¶„ì„ì„ ìœ„í•´ ì–¼êµ´ ì‚¬ì§„ 3ì¥ì´ í•„ìš”í•´ìš”.'),
        SizedBox(height: 12),
        Text('ì•ˆì‹¬í•˜ì„¸ìš”!', style: TextStyle(fontWeight: FontWeight.bold)),
        SizedBox(height: 8),
        Text('- ì‚¬ì§„ì€ ê¸°ê¸°ì—ì„œë§Œ ë¶„ì„ë˜ë©° ì„œë²„ì— ì „ì†¡ë˜ì§€ ì•Šì•„ìš”'),
        Text('- ë¶„ì„ í›„ ì‚¬ì§„ì€ ì¦‰ì‹œ ì‚­ì œë¼ìš”'),
        Text('- ì˜¤ì§ ë¶„ì„ ê²°ê³¼(ìˆ˜ì¹˜)ë§Œ ì•ˆì „í•˜ê²Œ ì €ì¥ë¼ìš”'),
      ],
    ),
    actions: [
      TextButton(onPressed: () => Navigator.pop(context), child: const Text('ì·¨ì†Œ')),
      FilledButton(onPressed: _startCapture, child: const Text('ì‹œì‘í•˜ê¸°')),
    ],
  );
}
```

---

## 11. Key Code Snippets

### FaceAnalyzerService (ML Kit Wrapper)

```dart
/// lib/features/gwansang/domain/services/face_analyzer_service.dart

import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:image_picker/image_picker.dart';

class FaceAnalyzerService {
  late final FaceDetector _detector;

  FaceAnalyzerService() {
    _detector = FaceDetector(
      options: FaceDetectorOptions(
        enableClassification: true,
        enableLandmarks: true,
        enableContours: true,
        enableTracking: false,
        performanceMode: FaceDetectorMode.accurate,
        minFaceSize: 0.25,
      ),
    );
  }

  /// Analyze a single photo and extract face data
  Future<FaceAnalysisResult?> analyzePhoto(XFile photo) async {
    final inputImage = InputImage.fromFilePath(photo.path);
    final faces = await _detector.processImage(inputImage);

    if (faces.isEmpty) return null;
    if (faces.length > 1) throw MultipleFacesException();

    final face = faces.first;

    return FaceAnalysisResult(
      boundingBox: face.boundingBox,
      landmarks: {
        for (final type in FaceLandmarkType.values)
          if (face.landmarks[type] != null)
            type: face.landmarks[type]!.position,
      },
      contours: {
        for (final type in FaceContourType.values)
          if (face.contours[type] != null)
            type: face.contours[type]!.points,
      },
      headEulerAngleX: face.headEulerAngleX,
      headEulerAngleY: face.headEulerAngleY,
      headEulerAngleZ: face.headEulerAngleZ,
      smilingProbability: face.smilingProbability,
      leftEyeOpenProbability: face.leftEyeOpenProbability,
      rightEyeOpenProbability: face.rightEyeOpenProbability,
    );
  }

  /// Dispose of the face detector
  void dispose() {
    _detector.close();
  }
}
```

### GwansangComputer (Pure Dart Measurement Engine)

```dart
/// lib/features/gwansang/domain/services/gwansang_computer.dart

import 'dart:math';

class GwansangComputer {
  /// Compute all ê´€ìƒ measurements from face analysis results
  GwansangMeasurements compute({
    required List<FaceAnalysisResult> faceResults,
    int primaryPhotoIndex = 0,
  }) {
    final primary = faceResults[primaryPhotoIndex];
    final faceContour = primary.contours[FaceContourType.face]!;
    final bbox = primary.boundingBox;

    return GwansangMeasurements(
      faceShape: _classifyFaceShape(faceContour, bbox),
      samJeong: _computeSamJeong(primary),
      eyes: _computeEyeMeasurements(primary),
      eyebrows: _computeEyebrowMeasurements(primary),
      nose: _computeNoseMeasurements(primary),
      mouth: _computeMouthMeasurements(primary),
      forehead: _computeForeheadMeasurements(primary),
      jawline: _computeJawlineMeasurements(primary),
      symmetry: _computeSymmetry(primary),
      proportions: _computeProportions(primary),
    );
  }

  // --- Face Shape ---

  FaceShapeResult _classifyFaceShape(
    List<Point<int>> faceContour,
    Rect bbox,
  ) {
    final width = bbox.width;
    final height = bbox.height;
    final ratio = width / height;

    // Measure width at different vertical positions
    final jawWidth = _widthAtPercentage(faceContour, 0.80);
    final cheekWidth = _widthAtPercentage(faceContour, 0.50);
    final foreheadWidth = _widthAtPercentage(faceContour, 0.20);

    final jawToForehead = jawWidth / foreheadWidth;
    final cheekToJaw = cheekWidth / jawWidth;

    FaceShapeType type;
    double confidence;

    if (ratio > 0.85 && jawToForehead > 0.95) {
      type = FaceShapeType.square;
      confidence = 0.7 + (jawToForehead - 0.95) * 2;
    } else if (ratio > 0.80 && jawToForehead > 0.85) {
      type = FaceShapeType.round;
      confidence = 0.7 + (ratio - 0.80) * 3;
    } else if (ratio < 0.68) {
      type = FaceShapeType.long;
      confidence = 0.7 + (0.68 - ratio) * 5;
    } else if (jawToForehead < 0.75) {
      type = FaceShapeType.heart;
      confidence = 0.7 + (0.75 - jawToForehead) * 3;
    } else if (cheekToJaw > 1.15) {
      type = FaceShapeType.diamond;
      confidence = 0.6 + (cheekToJaw - 1.15) * 3;
    } else {
      type = FaceShapeType.oval;
      confidence = 0.75;
    }

    return FaceShapeResult(
      type: type,
      confidence: confidence.clamp(0.0, 1.0),
      widthHeightRatio: ratio,
      jawForeheadRatio: jawToForehead,
    );
  }

  // --- Eyes ---

  EyeMeasurements _computeEyeMeasurements(FaceAnalysisResult face) {
    final leftEye = face.contours[FaceContourType.leftEye]!;
    final rightEye = face.contours[FaceContourType.rightEye]!;
    final faceWidth = face.boundingBox.width;

    // Eye width = distance between leftmost and rightmost points
    final leftEyeWidth = _contourWidth(leftEye);
    final rightEyeWidth = _contourWidth(rightEye);

    // Eye spacing = distance between inner corners
    final leftInner = leftEye.last;   // inner corner of left eye
    final rightInner = rightEye.first; // inner corner of right eye
    final eyeSpacing = _distance(leftInner, rightInner);

    // Slant angle = angle from inner to outer corner
    final leftSlant = _slantAngle(leftEye);
    final rightSlant = _slantAngle(rightEye);

    // Eye height (opening)
    final leftHeight = _contourHeight(leftEye);
    final rightHeight = _contourHeight(rightEye);

    return EyeMeasurements(
      leftWidth: leftEyeWidth,
      rightWidth: rightEyeWidth,
      spacingRatio: eyeSpacing / faceWidth,
      slantAngleLeft: leftSlant,
      slantAngleRight: rightSlant,
      sizeCategory: _categorizeEyeSize(leftEyeWidth / faceWidth),
      shape: _categorizeEyeShape(leftEyeWidth / leftHeight, leftSlant),
      openRatioLeft: leftHeight / leftEyeWidth,
      openRatioRight: rightHeight / rightEyeWidth,
      symmetryScore: _symmetryScore(leftEyeWidth, rightEyeWidth),
    );
  }

  // --- Utility Methods ---

  double _contourWidth(List<Point<int>> contour) {
    final xs = contour.map((p) => p.x);
    return (xs.reduce(max) - xs.reduce(min)).toDouble();
  }

  double _contourHeight(List<Point<int>> contour) {
    final ys = contour.map((p) => p.y);
    return (ys.reduce(max) - ys.reduce(min)).toDouble();
  }

  double _distance(Point<int> a, Point<int> b) {
    return sqrt(pow(a.x - b.x, 2) + pow(a.y - b.y, 2));
  }

  double _slantAngle(List<Point<int>> eyeContour) {
    // Angle from inner corner to outer corner
    final inner = eyeContour.last;
    final outer = eyeContour.first;
    return atan2((outer.y - inner.y).toDouble(), (outer.x - inner.x).toDouble()) * 180 / pi;
  }

  double _widthAtPercentage(List<Point<int>> contour, double percentage) {
    final ys = contour.map((p) => p.y);
    final minY = ys.reduce(min);
    final maxY = ys.reduce(max);
    final targetY = minY + (maxY - minY) * percentage;

    // Find points closest to targetY
    final nearPoints = contour.where((p) => (p.y - targetY).abs() < (maxY - minY) * 0.05).toList();
    if (nearPoints.length < 2) return 0;

    final xs = nearPoints.map((p) => p.x);
    return (xs.reduce(max) - xs.reduce(min)).toDouble();
  }

  int _symmetryScore(double left, double right) {
    final diff = (left - right).abs();
    final avg = (left + right) / 2;
    if (avg == 0) return 100;
    return ((1 - diff / avg) * 100).round().clamp(0, 100);
  }
}
```

### Edge Function Skeleton (Supabase)

```typescript
// supabase/functions/generate-gwansang-insight/index.ts

import { serve } from "https://deno.land/std@0.177.0/http/server.ts";
import Anthropic from "npm:@anthropic-ai/sdk";

const anthropic = new Anthropic({
  apiKey: Deno.env.get("ANTHROPIC_API_KEY"),
});

const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ í•œêµ­ ì „í†µ ê´€ìƒí•™(è§€ç›¸å­¸)ì— ì •í†µí•œ AI ê´€ìƒê°€ì…ë‹ˆë‹¤.
... (full system prompt as defined in Section 6)
`;

serve(async (req: Request) => {
  const { faceMeasurements, sajuProfile, userName } = await req.json();

  // Validate input
  if (!faceMeasurements || !sajuProfile) {
    return new Response(JSON.stringify({ error: "Missing required fields" }), {
      status: 400,
    });
  }

  // Construct user prompt
  const userPrompt = `
## ê´€ìƒ ë¶„ì„ ìš”ì²­

### ì–¼êµ´ ì¸¡ì • ë°ì´í„°
${JSON.stringify(faceMeasurements, null, 2)}

### ì‚¬ì£¼ í”„ë¡œí•„
- ì‚¬ìš©ì: ${userName || "ì‚¬ìš©ì"}
- ì—°ì£¼: ${sajuProfile.yearPillar}
- ì›”ì£¼: ${sajuProfile.monthPillar}
- ì¼ì£¼: ${sajuProfile.dayPillar}
- ì‹œì£¼: ${sajuProfile.hourPillar || "ë¯¸ìƒ"}
- ì˜¤í–‰ ë¶„í¬: ëª©(${sajuProfile.fiveElements.wood}) í™”(${sajuProfile.fiveElements.fire}) í† (${sajuProfile.fiveElements.earth}) ê¸ˆ(${sajuProfile.fiveElements.metal}) ìˆ˜(${sajuProfile.fiveElements.water})
- ì£¼ë„ ì˜¤í–‰: ${sajuProfile.dominantElement}

### ë¶„ì„ ìš”ì²­
ìœ„ ì–¼êµ´ ì¸¡ì • ë°ì´í„°ì™€ ì‚¬ì£¼ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ê´€ìƒ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
`;

  const response = await anthropic.messages.create({
    model: "claude-haiku-4-5-20250710",
    max_tokens: 2048,
    system: [
      {
        type: "text",
        text: SYSTEM_PROMPT,
        cache_control: { type: "ephemeral" },
      },
    ],
    messages: [{ role: "user", content: userPrompt }],
  });

  // Parse structured JSON from response
  const content = response.content[0].type === "text" ? response.content[0].text : "";

  let parsed;
  try {
    // Extract JSON from response (may be wrapped in markdown code blocks)
    const jsonMatch = content.match(/\{[\s\S]*\}/);
    parsed = jsonMatch ? JSON.parse(jsonMatch[0]) : null;
  } catch {
    parsed = {
      gwansang_reading: content,
      face_traits: [],
      combined_insight: "",
      love_fortune: "",
      compatibility_hints: [],
    };
  }

  return new Response(JSON.stringify(parsed), {
    headers: { "Content-Type": "application/json" },
  });
});
```

---

## 12. Risk Analysis & Mitigations

### Technical Risks

| Risk | Severity | Probability | Mitigation |
|---|---|---|---|
| ML Kit contour data insufficient for some face shapes | Medium | Low | Test with diverse face photos during development; supplement with landmark positions |
| Face detection fails in poor lighting | Medium | Medium | Comprehensive quality gate + guidance UI; allow flash/light boost |
| Inconsistent measurements between photos | Medium | Medium | Use primary (frontal) photo for all key measurements; side photos only for supplementary data |
| Claude output format unpredictable | Low | Low | JSON extraction with regex fallback; structured prompt with strict output format |
| ML Kit model size impacts app download | Low | Low | google_mlkit uses dynamic model download (not bundled); first run downloads ~5MB |

### Product Risks

| Risk | Severity | Mitigation |
|---|---|---|
| Users uncomfortable with face photo | High | Strong privacy messaging; option to skip ê´€ìƒ |
| ê´€ìƒ results feel generic/inauthentic | High | Detailed prompt engineering; combine with saju for personalization |
| Results feel negative/judgmental | High | Strict prompt guidelines: always positive/empowering tone |
| Feature adds too much friction to onboarding | Medium | Make ê´€ìƒ optional; or move to post-onboarding engagement feature |

### Platform-Specific Considerations

| Platform | Issue | Solution |
|---|---|---|
| iOS | Camera permission prompt | Pre-explain why photos needed before permission dialog |
| iOS | App Store review (face analysis) | Privacy policy must disclose on-device ML processing |
| Android | ML Kit model download on first use | Show download progress; cache model |
| Both | Large image memory usage (3 photos) | Process sequentially, dispose after each; compress to 1080px max |

---

## Summary

This architecture achieves:

1. **Privacy-first**: Zero photo upload -- all face analysis runs on-device via Google ML Kit
2. **Cross-platform**: `google_mlkit_face_detection` supports both iOS and Android
3. **Cost-efficient**: ~$0.007 per user (Haiku 4.5), scaling linearly
4. **Coherent ì‚¬ì£¼+ê´€ìƒ**: Single unified reading, not two separate analyses
5. **Fast**: ~4-5.5 seconds total processing time (excluding user photo capture)
6. **Clean Architecture**: Follows existing codebase patterns (feature-first, DI via core/di/providers.dart, Riverpod state management)
7. **Extensible**: Measurement JSON is versioned; new facial features can be added without breaking existing profiles

### Next Steps (Implementation Order)

1. Add packages to `pubspec.yaml` (`google_mlkit_face_detection`, `image_picker`, `image_cropper`, `image`, `crypto`)
2. Create domain entities: `FaceAnalysisResult`, `GwansangMeasurements`, `GwansangProfile`
3. Implement `FaceAnalyzerService` (ML Kit wrapper)
4. Implement `GwansangComputer` (measurement computation -- most complex piece)
5. Implement `PhotoQualityValidator`
6. Create Supabase Edge Function `generate-gwansang-insight`
7. Create DB table `gwansang_profiles` with RLS
8. Implement `GwansangRemoteDatasource` and `GwansangRepositoryImpl`
9. Build UI: capture page, analysis page, result page
10. Integrate into onboarding flow
11. Test with diverse face types and lighting conditions
